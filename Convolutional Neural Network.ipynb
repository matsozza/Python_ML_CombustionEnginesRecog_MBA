{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load necessary libraries ###\n",
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug and test variables\n",
    "REDUCED_MODE = 1  # Do not process all the audio in the dataset (for faster execution during script development)\n",
    "REDUCED_MODE_AUDIONUM = 20 # Number of dataset audios per folder to process in the reduced-mode.\n",
    "\n",
    "# Audio processing variables\n",
    "AUDIO_SR = 44100 # Audio sampling in Hertz\n",
    "AUDIO_N_FFT = 2048 # Samples per FFT window\n",
    "AUDIO_HOP_LEN = 512\n",
    "AUDIO_N_MELS = 60\n",
    "AUDIOSEG_SIZE = 41 # windows per subAudio / segment\n",
    "AUDIOSEG_OVERLAP = 0.5 # normalized\n",
    "\n",
    "# Paths and folders\n",
    "audiosPath = 'UrbanSounds8K/1_audioFiles/'\n",
    "processedPath = \"UrbanSounds8K/2_featuresProcessed/\"\n",
    "confusionPath = \"UrbanSounds8K/3_confusionMatrix/\"\n",
    "confusionFile = \"confusion_matrix\"\n",
    "audiosSubPaths = np.array(['fold1','fold2','fold3','fold4','fold5','fold6','fold7','fold8','fold9','fold10'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide an audio file into shorter parts.\n",
    "def audioSegmentation(audioData, subAudioSize, overlapFactor = AUDIOSEG_OVERLAP):\n",
    "    start = 0\n",
    "    while start < len(audioData):\n",
    "        yield int(start), int(start + subAudioSize)\n",
    "        start += (subAudioSize // (1/(1-overlapFactor)))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define helper functions ###\n",
    "def extractSoundFeature(audiosPath,audiosSubPath,audiosExtension=\"*.wav\", n_mels=AUDIO_N_MELS, n_windows=AUDIOSEG_SIZE, hopLength = AUDIO_HOP_LEN):\n",
    "    \n",
    "    subAudioSize = hopLength * (n_windows - 1)\n",
    "    features_AM, classes_A = [], []\n",
    "    \n",
    "    # Map all audio samples paths inside the folder\n",
    "    allAudiosPath_A = glob.glob(os.path.join(audiosPath, audiosSubPath, audiosExtension));\n",
    "\n",
    "    # Iterate and extract features from each audio\n",
    "    idxAudio = 0\n",
    "    idxSegment = 0\n",
    "    for audioPath in allAudiosPath_A:\n",
    "        idxAudio=idxAudio+1;\n",
    "        # Early stop for reduced mode\n",
    "        if REDUCED_MODE==1 and idxAudio == REDUCED_MODE_AUDIONUM:\n",
    "            break;\n",
    "        \n",
    "        subAudioLogSpect_AM, subAudioClass = [], [] # Init arrays\n",
    "\n",
    "        # Extract '.wav' audio to an 1-D array + audio classification\n",
    "        audioData_A, sr = librosa.load(audioPath, sr = AUDIO_SR)\n",
    "        audioClass = int(audioPath.split('/')[2].split('-')[1])\n",
    "\n",
    "        # Loop and extract all audio features from subsegments\n",
    "        for (start,end) in audioSegmentation(audioData_A, subAudioSize):\n",
    "            if(len(audioData_A[start:end]) == subAudioSize):\n",
    "                idxSegment = idxSegment+1;\n",
    "                # Append audio class to segments class array\n",
    "                subAudioClass.append(audioClass)\n",
    "                \n",
    "                #Extract audio data - 1D Array\n",
    "                segAudioData_A = audioData_A[start:end]\n",
    "                \n",
    "                # Transform to spectogram in decibels - 2D Matrix\n",
    "                segAudioSpect_M = librosa.feature.melspectrogram(y=segAudioData_A, n_mels=n_mels)\n",
    "                segAudioSpectDecibels_M = librosa.amplitude_to_db(segAudioSpect_M)\n",
    "\n",
    "                # Reshape and append to preprocessed spectograms\n",
    "                segAudioSpectDecibels_M = segAudioSpectDecibels_M.T.flatten()[:, np.newaxis].T\n",
    "                subAudioLogSpect_AM.append(segAudioSpectDecibels_M)\n",
    "\n",
    "        subAudioLogSpect_AM = np.asarray(subAudioLogSpect_AM).reshape(len(subAudioLogSpect_AM),n_mels,n_windows,1)\n",
    "\n",
    "        # If last audio not empty, stack together with others\n",
    "        if len(subAudioLogSpect_AM) > 0: \n",
    "            features_AM.append(subAudioLogSpect_AM)\n",
    "            classes_A.append(subAudioClass)\n",
    "    \n",
    "    print('Num of segments for this folder - ', idxSegment, ' - out of - ', idxAudio , ' audios.');\n",
    "\n",
    "    # Stack all data 'per segment' and not 'per audio'\n",
    "    features_AM = np.concatenate(features_AM, axis=0)\n",
    "    classes_A = np.concatenate(classes_A, axis=0) \n",
    "\n",
    "    # Convert UrbanSounds8K class to binary 'is/isn't engine'\n",
    "    engineClasses_A = np.array(list(map(lambda x: 1 if x==5 else 0, classes_A))) # @TODO Still not used\n",
    "\n",
    "    return features_AM, classes_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model Definition - with ReLu activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define convolutional network architecture ###\n",
    "def CNN_modelDefinition(useLeakyRelu = 0):\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    num_filters = [24,32,64,128] \n",
    "    pool_size = (2, 2) \n",
    "    kernel_size = (3, 3)  \n",
    "    input_shape = (60, 41, 1)\n",
    "    num_classes = 10\n",
    "    \n",
    "    # ---------- Convolutional + Pooling Layer 1 ---------- #\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(80, kernel_size, padding=\"same\", input_shape=input_shape, dilation_rate= 1))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    if useLeakyRelu == 0:\n",
    "        model.add(keras.layers.Activation(\"relu\"))\n",
    "    else:\n",
    "        model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    #model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "    # ---------- Convolutional + Pooling Layer 2 ---------- #\n",
    "    model.add(keras.layers.Conv2D(80, kernel_size, padding=\"same\", dilation_rate=2))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    if useLeakyRelu == 0:\n",
    "        model.add(keras.layers.Activation(\"relu\"))\n",
    "    else:\n",
    "        model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    \n",
    "    # ---------- Pooling Layer Layer ---------- #\n",
    "    model.add(keras.layers.GlobalMaxPooling2D())\n",
    "\n",
    "    # ---------- Flat Layers ---------- #\n",
    "    model.add(keras.layers.Dense(512, activation=\"relu\"))\n",
    "    #model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-4), loss=keras.losses.SparseCategoricalCrossentropy(), \n",
    "                   metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Processing and Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of segments for this folder -  288  - out of -  20  audios.\n",
      "Num of segments for this folder -  290  - out of -  20  audios.\n",
      "Num of segments for this folder -  293  - out of -  20  audios.\n",
      "Num of segments for this folder -  279  - out of -  20  audios.\n",
      "Num of segments for this folder -  288  - out of -  20  audios.\n",
      "Num of segments for this folder -  304  - out of -  20  audios.\n",
      "Num of segments for this folder -  304  - out of -  20  audios.\n",
      "Num of segments for this folder -  212  - out of -  20  audios.\n",
      "Num of segments for this folder -  304  - out of -  20  audios.\n",
      "Num of segments for this folder -  209  - out of -  20  audios.\n"
     ]
    }
   ],
   "source": [
    "# Pre-process and extract feature from the data\n",
    "for audioSubPath in audiosSubPaths:\n",
    "    features_AM, classes_A = extractSoundFeature(audiosPath,audioSubPath)\n",
    "    np.savez(\"{0}{1}\".format(processedPath, audioSubPath), features=features_AM, classes=classes_A);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training and Metrics Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Starting a loop! - Folder  1\n",
      "-> Getting data from TRAINING folder  2\n",
      "-> Getting data from TRAINING folder  3\n",
      "-> Getting data from TRAINING folder  4\n",
      "-> Getting data from TRAINING folder  5\n",
      "-> Getting data from TRAINING folder  6\n",
      "-> Getting data from TRAINING folder  7\n",
      "-> Getting data from TRAINING folder  8\n",
      "-> Getting data from TRAINING folder  9\n",
      "-> Getting data from TRAINING folder  10\n",
      "-> TRAIN dataset size:  2483\n",
      "\n",
      "-> Getting data from TESTING folder  [1]\n",
      "-> TEST dataset size:  288\n",
      "\n",
      "--> Fitting model!\n",
      "Epoch 1/12\n",
      "104/104 [==============================] - 9s 83ms/step - loss: 1.5086 - accuracy: 0.5123 - val_loss: 2.9326 - val_accuracy: 0.1042\n",
      "Epoch 2/12\n",
      "104/104 [==============================] - 8s 78ms/step - loss: 0.9113 - accuracy: 0.7511 - val_loss: 2.6533 - val_accuracy: 0.2708\n",
      "Epoch 3/12\n",
      "104/104 [==============================] - 8s 79ms/step - loss: 0.6296 - accuracy: 0.8602 - val_loss: 2.4945 - val_accuracy: 0.4062\n",
      "Epoch 4/12\n",
      "104/104 [==============================] - 8s 81ms/step - loss: 0.4419 - accuracy: 0.9178 - val_loss: 2.6954 - val_accuracy: 0.3819\n",
      "Epoch 5/12\n",
      "104/104 [==============================] - 8s 80ms/step - loss: 0.3412 - accuracy: 0.9412 - val_loss: 2.8426 - val_accuracy: 0.3576\n",
      "Epoch 6/12\n",
      "104/104 [==============================] - 9s 84ms/step - loss: 0.2667 - accuracy: 0.9513 - val_loss: 2.7883 - val_accuracy: 0.3715\n",
      "Epoch 7/12\n",
      "104/104 [==============================] - 8s 80ms/step - loss: 0.2148 - accuracy: 0.9605 - val_loss: 2.7104 - val_accuracy: 0.3229\n",
      "Epoch 8/12\n",
      "104/104 [==============================] - 8s 81ms/step - loss: 0.1742 - accuracy: 0.9678 - val_loss: 2.4943 - val_accuracy: 0.3993\n",
      "Epoch 9/12\n",
      "104/104 [==============================] - 8s 79ms/step - loss: 0.1511 - accuracy: 0.9722 - val_loss: 2.6150 - val_accuracy: 0.3993\n",
      "Epoch 10/12\n",
      "104/104 [==============================] - 8s 80ms/step - loss: 0.1204 - accuracy: 0.9799 - val_loss: 3.1268 - val_accuracy: 0.3611\n",
      "Epoch 11/12\n",
      "104/104 [==============================] - 9s 85ms/step - loss: 0.1061 - accuracy: 0.9819 - val_loss: 2.8163 - val_accuracy: 0.3854\n",
      "Epoch 12/12\n",
      "104/104 [==============================] - 9s 84ms/step - loss: 0.0879 - accuracy: 0.9855 - val_loss: 3.1192 - val_accuracy: 0.2847\n"
     ]
    }
   ],
   "source": [
    "### Train and evaluate via 10-audiosSubPaths cross-validation ###\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "allConfusionMatrix_AM = [] # List containing all confusion matrix for all K-Fold Train-Test\n",
    "allFitResults_A = []\n",
    "\n",
    "# Loop to traint-test in all folder combinations.\n",
    "idx = 0\n",
    "for train_index, test_index in kf.split(audiosSubPaths):\n",
    "    idx= idx+1\n",
    "    \n",
    "    print('--> Starting a loop! - Folder ', idx)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "\n",
    "    # ---------------- STEP 1 ----------------\n",
    "    # Load training data from 9 out of 10 folders \n",
    "    # Loop through all training folders and gather data in single feature array\n",
    "    for idxTrainFolder in train_index:\n",
    "        print('-> Getting data from TRAINING folder ', idxTrainFolder+1)\n",
    "        # Read pre-saved features or segments of an audio file (pre-processed)\n",
    "        train_data = np.load(\"{0}/{1}.npz\".format(processedPath,audiosSubPaths[idxTrainFolder]), allow_pickle=True)\n",
    "\n",
    "        # Append all the 'features' and 'classes' train datasets \n",
    "        # in a single list containing all train folders data\n",
    "        x_train.append(train_data[\"features\"])\n",
    "        y_train.append(train_data[\"classes\"])\n",
    "\n",
    "    # Stack all the folders in a single X/Y dataset\n",
    "    x_train = np.concatenate(x_train, axis = 0).astype(np.float32)\n",
    "    y_train = np.concatenate(y_train, axis = 0).astype(np.float32)\n",
    "    print('-> TRAIN dataset size: ', len(x_train));\n",
    "    \n",
    "    # ---------------- STEP 2 ----------------\n",
    "    # Load test data from 1 out of 10 folders \n",
    "    # Load test data from the test folder\n",
    "    print('\\n-> Getting data from TESTING folder ', test_index+1)\n",
    "    test_data = np.load(\"{0}/{1}.npz\".format(processedPath, audiosSubPaths[test_index][0]), allow_pickle=True)\n",
    "    x_test = test_data[\"features\"]\n",
    "    y_test = test_data[\"classes\"]\n",
    "    print('-> TEST dataset size: ', len(x_test));\n",
    "\n",
    "    CNN_model = CNN_modelDefinition(useLeakyRelu=1)\n",
    "    \n",
    "    print('\\n--> Fitting model!')\n",
    "    allFitResults_A.append(CNN_model.fit(x_train, y_train,validation_data=(x_test, y_test), epochs = 12, batch_size = 24, verbose = 1))\n",
    "    \n",
    "    # Predict results from test data\n",
    "    y_test_pred = [];\n",
    "    y_test_pred = CNN_model.predict(x_test); # Categorical results\n",
    "    y_test_pred = np.asarray( tf.argmax(y_test_pred, axis=1)  ) # OHE to Category\n",
    "\n",
    "    # Early stop for reduced mode\n",
    "    if REDUCED_MODE == 1:\n",
    "        break;\n",
    "\n",
    "    # Append the confusion matrix of this K-Fold run to a matirx list\n",
    "    allConfusionMatrix_AM.append(confusion_matrix(y_test_pred, y_test))\n",
    "\n",
    "\n",
    "# Save the resulting confusion matrices\n",
    "np.savez(\"{0}{1}\".format(confusionPath, confusionFile), allConfusionMatrix=allConfusionMatrix_AM);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class identifiers according to UrbanSounds 8k :\n",
    "\n",
    "0 = air_conditioner\n",
    "1 = car_horn\n",
    "2 = children_playing\n",
    "3 = dog_bark\n",
    "4 = drilling\n",
    "5 = engine_idling\n",
    "6 = gun_shot\n",
    "7 = jackhammer\n",
    "8 = siren\n",
    "9 = street_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(allConfusionMatrix_AM[0], cmap='binary', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of samples per class\n",
    "np.asarray(classes_A).plot.hist(bins=12, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"classes\"].plot.hist(bins=12, alpha=0.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of data per class\n",
    "plt.hist(np.concatenate(test_data[\"classes\"], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3c40342076cd490130f9df38b51275377a8f6bece07be498ddb041bf7f78465"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
