{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load necessary libraries ###\n",
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug and test variables\n",
    "REDUCED_MODE = 0  # Do not process all the audio in the dataset (for faster execution during script development)\n",
    "REDUCED_MODE_AUDIONUM = 15 # Number of dataset audios per folder to process in the reduced-mode.\n",
    "\n",
    "# Audio processing variables\n",
    "AUDIO_SR = 44100 # Audio sampling in Hertz\n",
    "AUDIO_N_FFT = 2048 # Samples per FFT window\n",
    "AUDIO_HOP_LEN = 512\n",
    "AUDIO_N_MELS = 60\n",
    "AUDIOSEG_SIZE = 41 # windows per subAudio / segment\n",
    "AUDIOSEG_OVERLAP = 0.5 # normalized\n",
    "\n",
    "# Paths and folders\n",
    "audiosPath = 'UrbanSounds8K/1_audioFiles/'\n",
    "processedPath = \"UrbanSounds8K/2_featuresProcessed/\"\n",
    "confusionPath = \"UrbanSounds8K/3_confusionMatrix/\"\n",
    "confusionFile = \"confusion_matrix\"\n",
    "audiosSubPaths = np.array(['fold1','fold2','fold3','fold4','fold5','fold6','fold7','fold8','fold9','fold10'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide an audio file into shorter parts.\n",
    "def audioSegmentation(audioData, subAudioSize, overlapFactor = AUDIOSEG_OVERLAP):\n",
    "    start = 0\n",
    "    while start < len(audioData):\n",
    "        yield int(start), int(start + subAudioSize)\n",
    "        start += (subAudioSize // (1/(1-overlapFactor)))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define helper functions ###\n",
    "def extractSoundFeature(audiosPath,audiosSubPath,audiosExtension=\"*.wav\", n_mels=AUDIO_N_MELS, n_windows=AUDIOSEG_SIZE, hopLength = AUDIO_HOP_LEN):\n",
    "    \n",
    "    subAudioSize = hopLength * (n_windows - 1)\n",
    "    features_AM, classes_A = [], []\n",
    "    \n",
    "    # Map all audio samples paths inside the folder\n",
    "    allAudiosPath_A = glob.glob(os.path.join(audiosPath, audiosSubPath, audiosExtension));\n",
    "\n",
    "    # Iterate and extract features from each audio\n",
    "    idxAudio = 0\n",
    "    idxSegment = 0\n",
    "    for audioPath in allAudiosPath_A:\n",
    "        idxAudio=idxAudio+1;\n",
    "        # Early stop for reduced mode\n",
    "        if REDUCED_MODE==1 and idxAudio == REDUCED_MODE_AUDIONUM:\n",
    "            break;\n",
    "        \n",
    "        subAudioLogSpect_AM, subAudioClass = [], []\n",
    "\n",
    "        # Extract '.wav' audio to an 1-D array + audio classification\n",
    "        audioData_A, sr = librosa.load(audioPath, sr = AUDIO_SR)\n",
    "        audioClass = int(audioPath.split('/')[2].split('-')[1])\n",
    "\n",
    "        # Loop and extract all audio features from subsegments\n",
    "        for (start,end) in audioSegmentation(audioData_A, subAudioSize):\n",
    "            if(len(audioData_A[start:end]) == subAudioSize):\n",
    "                idxSegment = idxSegment+1;\n",
    "                # Append audio class to segments class array\n",
    "                subAudioClass.append(audioClass)\n",
    "                \n",
    "                #Extract audio data - 1D Array\n",
    "                segAudioData_A = audioData_A[start:end]\n",
    "                \n",
    "                # Transform to spectogram in decibels - 2D Matrix\n",
    "                segAudioSpect_M = librosa.feature.melspectrogram(y=segAudioData_A, n_mels=n_mels)\n",
    "                segAudioSpectDecibels_M = librosa.amplitude_to_db(segAudioSpect_M)\n",
    "\n",
    "                # Reshape and append to preprocessed spectograms\n",
    "                segAudioSpectDecibels_M = segAudioSpectDecibels_M.T.flatten()[:, np.newaxis].T\n",
    "                subAudioLogSpect_AM.append(segAudioSpectDecibels_M)\n",
    "                \n",
    "        subAudioLogSpect_AM = np.asarray(subAudioLogSpect_AM).reshape(len(subAudioLogSpect_AM),n_mels,n_windows,1)\n",
    "\n",
    "        if len(subAudioLogSpect_AM) > 0: # if not empty, concatenate in features / classes array\n",
    "            features_AM.append(subAudioLogSpect_AM)\n",
    "            classes_A.append(subAudioClass)\n",
    "    \n",
    "    print('Num of segments for this folder - ', idxSegment, ' - out of - ', idxAudio , ' audios.');\n",
    "\n",
    "    return features_AM, classes_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model Definition - with ReLu activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define convolutional network architecture ###\n",
    "def CNN_modelDefinition(useLeakyRelu = 0):\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    num_filters = [24,32,64,128] \n",
    "    pool_size = (2, 2) \n",
    "    kernel_size = (3, 3)  \n",
    "    input_shape = (60, 41, 1)\n",
    "    num_classes = 10\n",
    "    \n",
    "    # ---------- Convolutional + Pooling Layer 1 ---------- #\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(80, kernel_size, padding=\"same\", input_shape=input_shape, dilation_rate= 1))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    if useLeakyRelu == 0:\n",
    "        model.add(keras.layers.Activation(\"relu\"))\n",
    "    else:\n",
    "        model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    #model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "    # ---------- Convolutional + Pooling Layer 2 ---------- #\n",
    "    model.add(keras.layers.Conv2D(80, kernel_size, padding=\"same\", dilation_rate=2))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    if useLeakyRelu == 0:\n",
    "        model.add(keras.layers.Activation(\"relu\"))\n",
    "    else:\n",
    "        model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    \n",
    "    # ---------- Pooling Layer Layer ---------- #\n",
    "    model.add(keras.layers.GlobalMaxPooling2D())\n",
    "\n",
    "    # ---------- Flat Layers ---------- #\n",
    "    model.add(keras.layers.Dense(512, activation=\"relu\"))\n",
    "    #model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-4), loss=keras.losses.SparseCategoricalCrossentropy(), \n",
    "                   metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Processing and Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of segments for this folder -  12539  - out of -  873  audios.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SZM1JVL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:713: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of segments for this folder -  12419  - out of -  888  audios.\n",
      "Num of segments for this folder -  13465  - out of -  925  audios.\n",
      "Num of segments for this folder -  13975  - out of -  990  audios.\n",
      "Num of segments for this folder -  13132  - out of -  936  audios.\n",
      "Num of segments for this folder -  11721  - out of -  823  audios.\n",
      "Num of segments for this folder -  12146  - out of -  838  audios.\n",
      "Num of segments for this folder -  11406  - out of -  806  audios.\n",
      "Num of segments for this folder -  11779  - out of -  816  audios.\n",
      "Num of segments for this folder -  12027  - out of -  837  audios.\n"
     ]
    }
   ],
   "source": [
    "# Pre-process and extract feature from the data\n",
    "for audioSubPath in audiosSubPaths:\n",
    "    features_AM, classes_A = extractSoundFeature(audiosPath,audioSubPath)\n",
    "    np.savez(\"{0}{1}\".format(processedPath, audioSubPath), features=features_AM, classes=classes_A);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training and Metrics Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Starting a loop! - Folder  1\n",
      "-> Getting data from TRAINING folder  2\n",
      "-> Getting data from TRAINING folder  3\n",
      "-> Getting data from TRAINING folder  4\n",
      "-> Getting data from TRAINING folder  5\n",
      "-> Getting data from TRAINING folder  6\n",
      "-> Getting data from TRAINING folder  7\n",
      "-> Getting data from TRAINING folder  8\n",
      "-> Getting data from TRAINING folder  9\n",
      "-> Getting data from TRAINING folder  10\n",
      "-> TRAIN dataset size:  112070\n",
      "\n",
      "-> Getting data from TESTING folder  [1]\n",
      "-> TEST dataset size:  12539\n",
      "\n",
      "--> Fitting model!\n",
      "Epoch 1/12\n",
      " 291/4670 [>.............................] - ETA: 5:48 - loss: 1.8615 - accuracy: 0.3403"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SZM1JVL\\Documents\\PESSOAL - MBA DSA TCC\\Convolutional Neural Network.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X13sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m CNN_model \u001b[39m=\u001b[39m CNN_modelDefinition(useLeakyRelu\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X13sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m--> Fitting model!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m allFitResults_A\u001b[39m.\u001b[39mappend(CNN_model\u001b[39m.\u001b[39;49mfit(x_train, y_train,validation_data\u001b[39m=\u001b[39;49m(x_test, y_test), epochs \u001b[39m=\u001b[39;49m \u001b[39m12\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m24\u001b[39;49m, verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Predict results from test data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m y_test_pred \u001b[39m=\u001b[39m [];\n",
      "File \u001b[1;32mc:\\Users\\SZM1JVL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\SZM1JVL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1385\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\SZM1JVL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\SZM1JVL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\SZM1JVL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\SZM1JVL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\SZM1JVL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\SZM1JVL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\SZM1JVL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Train and evaluate via 10-audiosSubPaths cross-validation ###\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "allConfusionMatrix_AM = [] # List containing all confusion matrix for all K-Fold Train-Test\n",
    "allFitResults_A = []\n",
    "\n",
    "# Loop to traint-test in all folder combinations.\n",
    "idx = 0\n",
    "for train_index, test_index in kf.split(audiosSubPaths):\n",
    "    idx= idx+1\n",
    "    \n",
    "    print('--> Starting a loop! - Folder ', idx)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "\n",
    "    # ---------------- STEP 1 ----------------\n",
    "    # Load training data from 9 out of 10 folders \n",
    "    # Loop through all training folders and gather data in single feature array\n",
    "    for idxTrainFolder in train_index:\n",
    "        print('-> Getting data from TRAINING folder ', idxTrainFolder+1)\n",
    "        # Read pre-saved features or segments of an audio file (pre-processed)\n",
    "        train_data = np.load(\"{0}/{1}.npz\".format(processedPath,audiosSubPaths[idxTrainFolder]), allow_pickle=True)\n",
    "\n",
    "        # Get the 'features' and 'classes' from current train folder\n",
    "        features = np.concatenate(train_data[\"features\"], axis=0) \n",
    "        classes = np.concatenate(train_data[\"classes\"], axis=0)\n",
    "\n",
    "        # Append all the 'features' and 'classes' train datasets \n",
    "        # in a single list containing all train folders data\n",
    "        x_train.append(features)\n",
    "        y_train.append(classes)\n",
    "\n",
    "    # Stack all the segments as if they are 'individual' features\n",
    "    x_train = np.concatenate(x_train, axis = 0).astype(np.float32)\n",
    "    y_train = np.concatenate(y_train, axis = 0).astype(np.float32)\n",
    "    print('-> TRAIN dataset size: ', len(x_train));\n",
    "    \n",
    "    # ---------------- STEP 2 ----------------\n",
    "    # Load test data from 1 out of 10 folders \n",
    "    # Load test data from the test folder\n",
    "    print('\\n-> Getting data from TESTING folder ', test_index+1)\n",
    "    test_data = np.load(\"{0}/{1}.npz\".format(processedPath, audiosSubPaths[test_index][0]), allow_pickle=True)\n",
    "    x_test = np.concatenate(test_data[\"features\"], axis = 0).astype(np.float32)\n",
    "    y_test = np.concatenate(test_data[\"classes\"], axis = 0).astype(np.float32)\n",
    "    print('-> TEST dataset size: ', len(x_test));\n",
    "\n",
    "    CNN_model = CNN_modelDefinition(useLeakyRelu=1)\n",
    "    \n",
    "    print('\\n--> Fitting model!')\n",
    "    allFitResults_A.append(CNN_model.fit(x_train, y_train,validation_data=(x_test, y_test), epochs = 12, batch_size = 24, verbose = 1))\n",
    "    \n",
    "    # Predict results from test data\n",
    "    y_test_pred = [];\n",
    "    y_test_pred = CNN_model.predict(x_test); # Categorical results\n",
    "    y_test_pred = np.asarray( tf.argmax(y_test_pred, axis=1)  ) # OHE to Category\n",
    "\n",
    "    # Early stop for reduced mode\n",
    "    if REDUCED_MODE == 1:\n",
    "        break;\n",
    "\n",
    "    # Append the confusion matrix of this K-Fold run to a matirx list\n",
    "    allConfusionMatrix_AM.append(confusion_matrix(y_test_pred, y_test))\n",
    "\n",
    "\n",
    "# Save the resulting confusion matrices\n",
    "np.savez(\"{0}{1}\".format(confusionPath, confusionFile), allConfusionMatrix=allConfusionMatrix_AM);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class identifiers according to UrbanSounds 8k :\n",
    "\n",
    "0 = air_conditioner\n",
    "1 = car_horn\n",
    "2 = children_playing\n",
    "3 = dog_bark\n",
    "4 = drilling\n",
    "5 = engine_idling\n",
    "6 = gun_shot\n",
    "7 = jackhammer\n",
    "8 = siren\n",
    "9 = street_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SZM1JVL\\Documents\\PESSOAL - MBA DSA TCC\\Convolutional Neural Network.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(allConfusionMatrix_AM[\u001b[39m0\u001b[39;49m], cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m, interpolation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mcolorbar()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "plt.imshow(allConfusionMatrix_AM[0], cmap='binary', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SZM1JVL\\AppData\\Local\\Temp\\ipykernel_31388\\3883445798.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.asarray(classes_A).plot.hist(bins=12, alpha=0.5);\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SZM1JVL\\Documents\\PESSOAL - MBA DSA TCC\\Convolutional Neural Network.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Histogram of samples per class\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m np\u001b[39m.\u001b[39;49masarray(classes_A)\u001b[39m.\u001b[39;49mplot\u001b[39m.\u001b[39mhist(bins\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "# Histogram of samples per class\n",
    "np.asarray(classes_A).plot.hist(bins=12, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SZM1JVL\\Documents\\PESSOAL - MBA DSA TCC\\Convolutional Neural Network.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_data[\u001b[39m\"\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mplot\u001b[39m.\u001b[39mhist(bins\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "test_data[\"classes\"].plot.hist(bins=12, alpha=0.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([118.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,   0., 160.]),\n",
       " array([3. , 3.4, 3.8, 4.2, 4.6, 5. , 5.4, 5.8, 6.2, 6.6, 7. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of data per class\n",
    "plt.hist(np.concatenate(test_data[\"classes\"], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3c40342076cd490130f9df38b51275377a8f6bece07be498ddb041bf7f78465"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
