{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load necessary libraries ###\n",
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug and test variables\n",
    "REDUCED_MODE = 1  # Do not process all the audio in the dataset (for faster execution during script development)\n",
    "REDUCED_MODE_AUDIONUM = 150 # Number of dataset audios per folder to process in the reduced-mode.\n",
    "\n",
    "# Audio processing variables\n",
    "AUDIO_SR = 44100 # Audio sampling in Hertz\n",
    "AUDIO_N_FFT = 2048 # Samples per FFT window\n",
    "AUDIO_HOP_LEN = 512\n",
    "AUDIO_N_MELS = 60\n",
    "AUDIOSEG_SIZE = 41 # windows per subAudio / segment\n",
    "AUDIOSEG_OVERLAP = 0.5 # normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide an audio file into shorter parts.\n",
    "def audioSegmentation(audioData, subAudioSize, overlapFactor = AUDIOSEG_OVERLAP):\n",
    "    start = 0\n",
    "    while start < len(audioData):\n",
    "        yield int(start), int(start + subAudioSize)\n",
    "        start += (subAudioSize // (1/(1-overlapFactor)))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define helper functions ###\n",
    "def extractSoundFeature(audiosPath,audiosSubPath,audiosExtension=\"*.wav\", n_mels=AUDIO_N_MELS, n_windows=AUDIOSEG_SIZE, hopLength = AUDIO_HOP_LEN):\n",
    "    \n",
    "    subAudioSize = hopLength * (n_windows - 1)\n",
    "    features_AM, classes_A = [], []\n",
    "    \n",
    "    # Map all audio samples paths inside the folder\n",
    "    allAudiosPath_A = glob.glob(os.path.join(audiosPath, audiosSubPath, audiosExtension));\n",
    "\n",
    "    # Iterate and extract features from each audio\n",
    "    idxAudio = 0\n",
    "    idxSegment = 0\n",
    "    for audioPath in allAudiosPath_A:\n",
    "        idxAudio=idxAudio+1;\n",
    "        # Early stop for reduced mode\n",
    "        if REDUCED_MODE==1 and idxAudio == REDUCED_MODE_AUDIONUM:\n",
    "            break;\n",
    "        \n",
    "        subAudioLogSpect_AM, subAudioClass = [], []\n",
    "\n",
    "        # Extract '.wav' audio to an 1-D array + audio classification\n",
    "        audioData_A, sr = librosa.load(audioPath, sr = AUDIO_SR)\n",
    "        audioClass = int(audioPath.split('/')[2].split('-')[1])\n",
    "\n",
    "        # Loop and extract all audio features from subsegments\n",
    "        for (start,end) in audioSegmentation(audioData_A, subAudioSize):\n",
    "            if(len(audioData_A[start:end]) == subAudioSize):\n",
    "                idxSegment = idxSegment+1;\n",
    "                # Append audio class to segments class array\n",
    "                subAudioClass.append(audioClass)\n",
    "                \n",
    "                #Extract audio data - 1D Array\n",
    "                segAudioData_A = audioData_A[start:end]\n",
    "                \n",
    "                # Transform to spectogram in decibels - 2D Matrix\n",
    "                segAudioSpect_M = librosa.feature.melspectrogram(y=segAudioData_A, n_mels=n_mels)\n",
    "                segAudioSpectDecibels_M = librosa.amplitude_to_db(segAudioSpect_M)\n",
    "\n",
    "                # Reshape and append to preprocessed spectograms\n",
    "                segAudioSpectDecibels_M = segAudioSpectDecibels_M.T.flatten()[:, np.newaxis].T\n",
    "                subAudioLogSpect_AM.append(segAudioSpectDecibels_M)\n",
    "                \n",
    "        subAudioLogSpect_AM = np.asarray(subAudioLogSpect_AM).reshape(len(subAudioLogSpect_AM),n_mels,n_windows,1)\n",
    "\n",
    "        if len(subAudioLogSpect_AM) > 0: # if not empty, concatenate in features / classes array\n",
    "            features_AM.append(subAudioLogSpect_AM)\n",
    "            classes_A.append(subAudioClass)\n",
    "    \n",
    "    print('Num of segments for this folder - ', idxSegment, ' - out of - ', idxAudio , ' audios.');\n",
    "\n",
    "    return features_AM, classes_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define convolutional network architecture ###\n",
    "def CNN_modelDefinition():\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    num_filters = [24,32,64,128] \n",
    "    pool_size = (2, 2) \n",
    "    kernel_size = (3, 3)  \n",
    "    input_shape = (60, 41, 1)\n",
    "    num_classes = 10\n",
    "    \n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(24, kernel_size, padding=\"same\", input_shape=input_shape))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(32, kernel_size, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))  \n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(64, kernel_size, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))  \n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(128, kernel_size, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))  \n",
    "\n",
    "    model.add(keras.layers.GlobalMaxPooling2D())\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-4), \n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(), \n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of segments for this folder -  2318  - out of -  150  audios.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SZM1JVL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:713: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of segments for this folder -  2096  - out of -  150  audios.\n",
      "Num of segments for this folder -  2052  - out of -  150  audios.\n",
      "Num of segments for this folder -  2192  - out of -  150  audios.\n",
      "Num of segments for this folder -  2368  - out of -  150  audios.\n",
      "Num of segments for this folder -  2314  - out of -  150  audios.\n",
      "Num of segments for this folder -  2307  - out of -  150  audios.\n",
      "Num of segments for this folder -  2191  - out of -  150  audios.\n",
      "Num of segments for this folder -  2276  - out of -  150  audios.\n",
      "Num of segments for this folder -  2228  - out of -  150  audios.\n"
     ]
    }
   ],
   "source": [
    "# Pre-process and extract feature from the data\n",
    "audiosPath = 'UrbanSounds8K/1_audioFiles/'\n",
    "processedPath = \"UrbanSounds8K/2_featuresProcessed/\"\n",
    "audiosSubPaths = np.array(['fold1','fold2','fold3','fold4','fold5','fold6','fold7','fold8','fold9','fold10'])\n",
    "for audioSubPath in audiosSubPaths:\n",
    "    features_AM, classes_A = extractSoundFeature(audiosPath,audioSubPath)\n",
    "    np.savez(\"{0}{1}\".format(processedPath, audioSubPath), features=features_AM, classes=classes_A);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Starting a loop!\n",
      "-> Getting data from TRAINING folder  2\n",
      "-> Getting data from TRAINING folder  3\n",
      "-> Getting data from TRAINING folder  4\n",
      "-> Getting data from TRAINING folder  5\n",
      "-> Getting data from TRAINING folder  6\n",
      "-> Getting data from TRAINING folder  7\n",
      "-> Getting data from TRAINING folder  8\n",
      "-> Getting data from TRAINING folder  9\n",
      "-> Getting data from TRAINING folder  10\n",
      "-> TRAIN dataset size:  20024\n",
      "\n",
      "-> Getting data from TESTING folder  [1]\n",
      "-> TEST dataset size:  2318\n",
      "\n",
      "--> Fitting model!\n",
      "Epoch 1/12\n",
      "833/835 [============================>.] - ETA: 0s - loss: 0.8112 - accuracy: 0.7616"
     ]
    }
   ],
   "source": [
    "### Train and evaluate via 10-audiosSubPaths cross-validation ###\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "allConfusionMatrix_AM = [] # List containing all confusion matrix for all K-Fold Train-Test\n",
    "allFitResults_A = []\n",
    "\n",
    "# Loop to traint-test in all folder combinations.\n",
    "idx = 0\n",
    "for train_index, test_index in kf.split(audiosSubPaths):\n",
    "    idx= idx+1\n",
    "    \n",
    "    print('--> Starting a loop!')\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "\n",
    "    # ---------------- STEP 1 ----------------\n",
    "    # Load training data from 9 out of 10 folders \n",
    "    # Loop through all training folders and gather data in single feature array\n",
    "    for idxTrainFolder in train_index:\n",
    "        print('-> Getting data from TRAINING folder ', idxTrainFolder+1)\n",
    "        # Read pre-saved features or segments of an audio file (pre-processed)\n",
    "        train_data = np.load(\"{0}/{1}.npz\".format(processedPath,audiosSubPaths[idxTrainFolder]), allow_pickle=True)\n",
    "\n",
    "        # Get the 'features' and 'classes' from current train folder\n",
    "        features = np.concatenate(train_data[\"features\"], axis=0) \n",
    "        classes = np.concatenate(train_data[\"classes\"], axis=0)\n",
    "\n",
    "        # Append all the 'features' and 'classes' train datasets \n",
    "        # in a single list containing all train folders data\n",
    "        x_train.append(features)\n",
    "        y_train.append(classes)\n",
    "\n",
    "    # Stack all the segments as if they are 'individual' features\n",
    "    x_train = np.concatenate(x_train, axis = 0).astype(np.float32)\n",
    "    y_train = np.concatenate(y_train, axis = 0).astype(np.float32)\n",
    "    print('-> TRAIN dataset size: ', len(x_train));\n",
    "    \n",
    "    # ---------------- STEP 2 ----------------\n",
    "    # Load test data from 1 out of 10 folders \n",
    "    # Load test data from the test folder\n",
    "    print('\\n-> Getting data from TESTING folder ', test_index+1)\n",
    "    test_data = np.load(\"{0}/{1}.npz\".format(processedPath, audiosSubPaths[test_index][0]), allow_pickle=True)\n",
    "    x_test = np.concatenate(test_data[\"features\"], axis = 0).astype(np.float32)\n",
    "    y_test = np.concatenate(test_data[\"classes\"], axis = 0).astype(np.float32)\n",
    "    print('-> TEST dataset size: ', len(x_test));\n",
    "\n",
    "    CNN_model = CNN_modelDefinition()\n",
    "    \n",
    "    print('\\n--> Fitting model!')\n",
    "    allFitResults_A.append(CNN_model.fit(x_train, y_train,validation_data=(x_test, y_test), epochs = 12, batch_size = 24, verbose = 1))\n",
    "    \n",
    "    # Predict results from test data\n",
    "    y_test_pred = [];\n",
    "    y_test_pred = CNN_model.predict(x_test); # Categorical results\n",
    "    y_test_pred = np.asarray( tf.argmax(y_test_pred, axis=1)  ) # OHE to Category\n",
    "\n",
    "    # Early stop for reduced mode\n",
    "    if REDUCED_MODE == 1:\n",
    "        break;\n",
    "\n",
    "    # Append the confusion matrix of this K-Fold run to a matirx list\n",
    "    allConfusionMatrix_AM.append(confusion_matrix(y_test_pred, y_test))\n",
    "\n",
    "\n",
    "# Save the resulting confusion matrices\n",
    "confusionPath = \"UrbanSounds8K/3_confusionMatrix/\"\n",
    "confusionFile = \"confusion_matrix\"\n",
    "np.savez(\"{0}{1}\".format(confusionPath, confusionFile), allConfusionMatrix=allConfusionMatrix_AM);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class identifiers according to UrbanSounds 8k :\n",
    "\n",
    "0 = air_conditioner\n",
    "1 = car_horn\n",
    "2 = children_playing\n",
    "3 = dog_bark\n",
    "4 = drilling\n",
    "5 = engine_idling\n",
    "6 = gun_shot\n",
    "7 = jackhammer\n",
    "8 = siren\n",
    "9 = street_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SZM1JVL\\Documents\\PESSOAL - MBA DSA TCC\\Convolutional Neural Network.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(allConfusionMatrix_AM[\u001b[39m0\u001b[39;49m], cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m, interpolation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mcolorbar()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "plt.imshow(allConfusionMatrix_AM[0], cmap='binary', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SZM1JVL\\AppData\\Local\\Temp\\ipykernel_31388\\3883445798.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.asarray(classes_A).plot.hist(bins=12, alpha=0.5);\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SZM1JVL\\Documents\\PESSOAL - MBA DSA TCC\\Convolutional Neural Network.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Histogram of samples per class\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m np\u001b[39m.\u001b[39;49masarray(classes_A)\u001b[39m.\u001b[39;49mplot\u001b[39m.\u001b[39mhist(bins\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "# Histogram of samples per class\n",
    "np.asarray(classes_A).plot.hist(bins=12, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SZM1JVL\\Documents\\PESSOAL - MBA DSA TCC\\Convolutional Neural Network.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_data[\u001b[39m\"\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mplot\u001b[39m.\u001b[39mhist(bins\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "test_data[\"classes\"].plot.hist(bins=12, alpha=0.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([118.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,   0., 160.]),\n",
       " array([3. , 3.4, 3.8, 4.2, 4.6, 5. , 5.4, 5.8, 6.2, 6.6, 7. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of data per class\n",
    "plt.hist(np.concatenate(test_data[\"classes\"], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3c40342076cd490130f9df38b51275377a8f6bece07be498ddb041bf7f78465"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
