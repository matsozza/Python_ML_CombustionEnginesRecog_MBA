{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load necessary libraries ###\n",
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug and test variables\n",
    "REDUCED_MODE = 0  # Do not process all the audio in the dataset (for faster execution during script development)\n",
    "REDUCED_MODE_AUDIONUM = 300 # Number of dataset audios per folder to process in the reduced-mode.\n",
    "\n",
    "# Audio processing variables\n",
    "AUDIO_SR = 44100 # Audio sampling in Hertz\n",
    "AUDIO_N_FFT = 2048 # Samples per FFT window\n",
    "AUDIO_WINSIZE = 1024\n",
    "AUDIO_HOP_LEN = 512\n",
    "AUDIO_N_MELS = 60\n",
    "AUDIOSEG_SIZE = 41 # windows per subAudio / segment\n",
    "AUDIOSEG_OVERLAP = 0.5 # normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide an audio file into shorter parts.\n",
    "def audioSegmentation(audioData, subAudioSize, overlapFactor = AUDIOSEG_OVERLAP):\n",
    "    start = 0\n",
    "    while start < len(audioData):\n",
    "        yield int(start), int(start + subAudioSize)\n",
    "        start += ( subAudioSize // (1/(1-overlapFactor)) )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define helper functions ###\n",
    "def extractSoundFeature(audiosPath,audiosSubPath,audiosExtension=\"*.wav\", n_mels=AUDIO_N_MELS, n_windows=AUDIOSEG_SIZE, hopLength = AUDIO_HOP_LEN):\n",
    "    \n",
    "    subAudioSize = hopLength * (n_windows - 1)\n",
    "    features_AM, classes_A = [], []\n",
    "    \n",
    "    # Map all audio samples paths inside the folder\n",
    "    allAudiosPath_A = glob.glob(os.path.join(audiosPath, audiosSubPath, audiosExtension));\n",
    "\n",
    "    # Iterate and extract features from each audio\n",
    "    idx = 0\n",
    "    idxSegment = 0\n",
    "    for audioPath in allAudiosPath_A:\n",
    "        idx=idx+1;\n",
    "        # Early stop for reduced mode\n",
    "        if REDUCED_MODE==1 and idx == REDUCED_MODE_AUDIONUM:\n",
    "            break;\n",
    "        \n",
    "        subAudioLogSpect_AM, subAudioClass = [] , []\n",
    "\n",
    "        # Extract '.wav' audio to an 1-D array + audio classification\n",
    "        audioData_A, sr = librosa.load(audioPath, sr = AUDIO_SR)\n",
    "        audioClass = int(audioPath.split('/')[2].split('-')[1])\n",
    "\n",
    "        # Loop and extract all audio features from subsegments\n",
    "        \n",
    "        for (start,end) in audioSegmentation(audioData_A, subAudioSize):\n",
    "            \n",
    "            if(len(audioData_A[start:end]) == subAudioSize):\n",
    "                idxSegment = idxSegment+1;\n",
    "                # Append audio class to segments class array\n",
    "                subAudioClass.append(audioClass)\n",
    "                \n",
    "                #Extract audio data - 1D Array\n",
    "                segAudioData_A = audioData_A[start:end]\n",
    "                \n",
    "                # Transform to spectogram in decibels - 2D Matrix\n",
    "                segAudioSpect_M = librosa.feature.melspectrogram(y=segAudioData_A,n_mels=n_mels)\n",
    "                segAudioSpectDecibels_M = librosa.amplitude_to_db(segAudioSpect_M)\n",
    "\n",
    "                # Reshape and append to preprocessed spectograms\n",
    "                segAudioSpectDecibels_M = segAudioSpectDecibels_M.T.flatten()[:, np.newaxis].T\n",
    "                subAudioLogSpect_AM.append(segAudioSpectDecibels_M)\n",
    "                \n",
    "        subAudioLogSpect_AM = np.asarray(subAudioLogSpect_AM).reshape(len(subAudioLogSpect_AM),n_mels,n_windows,1)\n",
    "\n",
    "        if len(subAudioLogSpect_AM) > 0: # if not empty, concatenate in features / classes array\n",
    "            features_AM.append(subAudioLogSpect_AM)\n",
    "            classes_A.append(subAudioClass)\n",
    "    \n",
    "    print('num of segments for this folder: ', idxSegment);\n",
    "\n",
    "    return features_AM, classes_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define convolutional network architecture ###\n",
    "def CNN_modelDefinition():\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    num_filters = [24,32,64,128] \n",
    "    pool_size = (2, 2) \n",
    "    kernel_size = (3, 3)  \n",
    "    input_shape = (60, 41, 1)\n",
    "    num_classes = 10\n",
    "    \n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(24, kernel_size, padding=\"same\", input_shape=input_shape))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(32, kernel_size, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))  \n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(64, kernel_size, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))  \n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(128, kernel_size, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))  \n",
    "\n",
    "    model.add(keras.layers.GlobalMaxPooling2D())\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-4), \n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(), \n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of segments for this folder:  12539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SZM1JVL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:713: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of segments for this folder:  12419\n",
      "num of segments for this folder:  13465\n",
      "num of segments for this folder:  13975\n",
      "num of segments for this folder:  13132\n",
      "num of segments for this folder:  11721\n",
      "num of segments for this folder:  12146\n",
      "num of segments for this folder:  11406\n",
      "num of segments for this folder:  11779\n",
      "num of segments for this folder:  12027\n"
     ]
    }
   ],
   "source": [
    "# Pre-process and extract feature from the data\n",
    "audiosPath = 'UrbanSounds8K/audio/'\n",
    "processedPath = \"UrbanSounds8K/processed/\"\n",
    "audiosSubPaths = np.array(['fold1','fold2','fold3','fold4','fold5','fold6','fold7','fold8','fold9','fold10'])\n",
    "for audioSubPath in audiosSubPaths:\n",
    "    features_AM, classes_A = extractSoundFeature(audiosPath,audioSubPath)\n",
    "    np.savez(\"{0}{1}\".format(processedPath, audioSubPath), features=features_AM, labels=classes_A);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train and evaluate via 10-audioSubPaths cross-validation ###\n",
    "accuracies = []\n",
    "audioSubPaths = np.array(['fold1','fold2','fold3','fold4','fold5','fold6','fold7','fold8', 'fold9','fold10'])\n",
    "processedPath = \"UrbanSounds8K/processed/\"\n",
    "kf = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Starting a loop!\n",
      "-> Getting data from TRAINING folder  2\n",
      "-> Getting data from TRAINING folder  3\n",
      "-> Getting data from TRAINING folder  4\n",
      "-> Getting data from TRAINING folder  5\n",
      "-> Getting data from TRAINING folder  6\n",
      "-> Getting data from TRAINING folder  7\n",
      "-> Getting data from TRAINING folder  8\n",
      "-> Getting data from TRAINING folder  9\n",
      "-> Getting data from TRAINING folder  10\n"
     ]
    }
   ],
   "source": [
    "allConfusionMatrix_AM = [] # List containing all confusion matrix for all K-Fold Train-Test\n",
    "allFitResults_A = []\n",
    "\n",
    "# Loop to traint-test in all folder combinations.\n",
    "idx = 0\n",
    "for train_index, test_index in kf.split(audioSubPaths):\n",
    "    idx= idx+1\n",
    "    \n",
    "    print('--> Starting a loop!')\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "\n",
    "    # ---------------- STEP 1 ----------------\n",
    "    # Load training data from 9 out of 10 folders \n",
    "    # Loop through all training folders and gather data in single feature array\n",
    "    for idxTrainFolder in train_index:\n",
    "        print('-> Getting data from TRAINING folder ', idxTrainFolder+1)\n",
    "        # Read pre-saved features or segments of an audio file (pre-processed)\n",
    "        train_data = np.load(\"{0}/{1}.npz\".format(processedPath,audioSubPaths[idxTrainFolder]), allow_pickle=True)\n",
    "\n",
    "        # Get the 'features' and 'labels' from current train folder\n",
    "        features = np.concatenate(train_data[\"features\"], axis=0) \n",
    "        labels = np.concatenate(train_data[\"labels\"], axis=0)\n",
    "\n",
    "        # Append all the 'features' and 'labels' train datasets \n",
    "        # in a single list containing all train folders data\n",
    "        x_train.append(features)\n",
    "        y_train.append(labels)\n",
    "\n",
    "    # Stack all the segments as if they are 'individual' features\n",
    "    x_train = np.concatenate(x_train, axis = 0).astype(np.float32)\n",
    "    y_train = np.concatenate(y_train, axis = 0).astype(np.float32)\n",
    "    print('-> TRAIN dataset size: ', len(x_train));\n",
    "    \n",
    "    # ---------------- STEP 2 ----------------\n",
    "    # Load test data from 1 out of 10 folders \n",
    "    # Load test data from the test folder\n",
    "    print('-> Getting data from TESTING folder ', test_index+1)\n",
    "    test_data = np.load(\"{0}/{1}.npz\".format(processedPath, audioSubPaths[test_index][0]), allow_pickle=True)\n",
    "    x_test = np.concatenate(test_data[\"features\"], axis = 0).astype(np.float32)\n",
    "    y_test = np.concatenate(test_data[\"labels\"], axis = 0).astype(np.float32)\n",
    "    print('-> TEST dataset size: ', len(x_test));\n",
    "\n",
    "    CNN_model = CNN_modelDefinition()\n",
    "    \n",
    "    print('--> Fitting model!')\n",
    "    allFitResults_A.append(CNN_model.fit(x_train, y_train,validation_data=(x_test, y_test), epochs = 1, batch_size = 24, verbose = 1))\n",
    "    \n",
    "    # Predict results from test data\n",
    "    y_test_pred = [];\n",
    "    y_test_pred = CNN_model.predict(x_test); # Categorical results\n",
    "    y_test_pred = np.asarray( tf.argmax(y_test_pred, axis=1)  )\n",
    "\n",
    "\n",
    "    # Early stop for reduced mode\n",
    "    if REDUCED_MODE == 1:\n",
    "        break;\n",
    "\n",
    "    allConfusionMatrix_AM.append(confusion_matrix(y_test_pred, y_test))\n",
    "\n",
    "print(\"Average 10 audioSubPaths Accuracy: {0}\".format(np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class identifiers according to UrbanSounds 8k :\n",
    "\n",
    "0 = air_conditioner\n",
    "1 = car_horn\n",
    "2 = children_playing\n",
    "3 = dog_bark\n",
    "4 = drilling\n",
    "5 = engine_idling\n",
    "6 = gun_shot\n",
    "7 = jackhammer\n",
    "8 = siren\n",
    "9 = street_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allConfusionMatrix[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3c40342076cd490130f9df38b51275377a8f6bece07be498ddb041bf7f78465"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
