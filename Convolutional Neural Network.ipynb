{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load necessary libraries ###\n",
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug and test variables\n",
    "REDUCED_MODE = 1  # Do not process all the audio in the dataset (for faster execution during script development)\n",
    "REDUCED_MODE_AUDIONUM = 20 # Number of dataset audios per folder to process in the reduced-mode.\n",
    "\n",
    "# Audio processing variables\n",
    "AUDIO_SR = 44100 # Audio sampling in Hertz\n",
    "AUDIO_N_FFT = 2048 # Samples per FFT window\n",
    "AUDIO_WINSIZE = 1024\n",
    "AUDIO_HOP_LEN = 512\n",
    "AUDIO_N_MELS = 60\n",
    "AUDIOSEG_SIZE = 41 # windows per subAudio / segment\n",
    "AUDIOSEG_OVERLAP = 0.5 # normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide an audio file into shorter parts.\n",
    "def audioSegmentation(audioData, subAudioSize, overlapFactor = AUDIOSEG_OVERLAP):\n",
    "    start = 0\n",
    "    while start < len(audioData):\n",
    "        yield int(start), int(start + subAudioSize)\n",
    "        start += ( subAudioSize // (1/(1-overlapFactor)) )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define helper functions ###\n",
    "def extractSoundFeature(audiosPath,audiosSubPath,audiosExtension=\"*.wav\", n_mels=AUDIO_N_MELS, n_windows=AUDIOSEG_SIZE, hopLength = AUDIO_HOP_LEN):\n",
    "    \n",
    "    subAudioSize = hopLength * (n_windows - 1)\n",
    "    features_AM, classes_A = [], []\n",
    "    \n",
    "    # Map all audio samples paths inside the folder\n",
    "    allAudiosPath_A = glob.glob(os.path.join(audiosPath, audiosSubPath, audiosExtension));\n",
    "\n",
    "    # Iterate and extract features from each audio\n",
    "    idxAudio = 0\n",
    "    idxSegment = 0\n",
    "    for audioPath in allAudiosPath_A:\n",
    "        idxAudio=idxAudio+1;\n",
    "        # Early stop for reduced mode\n",
    "        if REDUCED_MODE==1 and idxAudio == REDUCED_MODE_AUDIONUM:\n",
    "            break;\n",
    "        \n",
    "        subAudioLogSpect_AM, subAudioClass = [], []\n",
    "\n",
    "        # Extract '.wav' audio to an 1-D array + audio classification\n",
    "        audioData_A, sr = librosa.load(audioPath, sr = AUDIO_SR)\n",
    "        audioClass = int(audioPath.split('/')[2].split('-')[1])\n",
    "\n",
    "        # Loop and extract all audio features from subsegments\n",
    "        for (start,end) in audioSegmentation(audioData_A, subAudioSize):\n",
    "            if(len(audioData_A[start:end]) == subAudioSize):\n",
    "                idxSegment = idxSegment+1;\n",
    "                # Append audio class to segments class array\n",
    "                subAudioClass.append(audioClass)\n",
    "                \n",
    "                #Extract audio data - 1D Array\n",
    "                segAudioData_A = audioData_A[start:end]\n",
    "                \n",
    "                # Transform to spectogram in decibels - 2D Matrix\n",
    "                segAudioSpect_M = librosa.feature.melspectrogram(y=segAudioData_A, n_mels=n_mels)\n",
    "                segAudioSpectDecibels_M = librosa.amplitude_to_db(segAudioSpect_M)\n",
    "\n",
    "                # Reshape and append to preprocessed spectograms\n",
    "                segAudioSpectDecibels_M = segAudioSpectDecibels_M.T.flatten()[:, np.newaxis].T\n",
    "                subAudioLogSpect_AM.append(segAudioSpectDecibels_M)\n",
    "                \n",
    "        subAudioLogSpect_AM = np.asarray(subAudioLogSpect_AM).reshape(len(subAudioLogSpect_AM),n_mels,n_windows,1)\n",
    "\n",
    "        if len(subAudioLogSpect_AM) > 0: # if not empty, concatenate in features / classes array\n",
    "            features_AM.append(subAudioLogSpect_AM)\n",
    "            classes_A.append(subAudioClass)\n",
    "    \n",
    "    print('Num of segments for this folder - ', idxSegment, ' - out of - ', idxAudio , ' audios.');\n",
    "\n",
    "    return features_AM, classes_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define convolutional network architecture ###\n",
    "def CNN_modelDefinition():\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    num_filters = [24,32,64,128] \n",
    "    pool_size = (2, 2) \n",
    "    kernel_size = (3, 3)  \n",
    "    input_shape = (60, 41, 1)\n",
    "    num_classes = 10\n",
    "    \n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(24, kernel_size, padding=\"same\", input_shape=input_shape))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(32, kernel_size, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))  \n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(64, kernel_size, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))  \n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(128, kernel_size, padding=\"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))  \n",
    "\n",
    "    model.add(keras.layers.GlobalMaxPooling2D())\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-4), \n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(), \n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of segments for this folder -  288  - out of -  20  audios.\n",
      "Num of segments for this folder -  290  - out of -  20  audios.\n",
      "Num of segments for this folder -  293  - out of -  20  audios.\n",
      "Num of segments for this folder -  279  - out of -  20  audios.\n",
      "Num of segments for this folder -  288  - out of -  20  audios.\n",
      "Num of segments for this folder -  304  - out of -  20  audios.\n",
      "Num of segments for this folder -  304  - out of -  20  audios.\n",
      "Num of segments for this folder -  212  - out of -  20  audios.\n",
      "Num of segments for this folder -  304  - out of -  20  audios.\n",
      "Num of segments for this folder -  209  - out of -  20  audios.\n"
     ]
    }
   ],
   "source": [
    "# Pre-process and extract feature from the data\n",
    "audiosPath = 'UrbanSounds8K/1_audioFiles/'\n",
    "processedPath = \"UrbanSounds8K/2_featuresProcessed/\"\n",
    "audiosSubPaths = np.array(['fold1','fold2','fold3','fold4','fold5','fold6','fold7','fold8','fold9','fold10'])\n",
    "for audioSubPath in audiosSubPaths:\n",
    "    features_AM, classes_A = extractSoundFeature(audiosPath,audioSubPath)\n",
    "    np.savez(\"{0}{1}\".format(processedPath, audioSubPath), features=features_AM, labels=classes_A);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Starting a loop!\n",
      "-> Getting data from TRAINING folder  2\n",
      "-> Getting data from TRAINING folder  3\n",
      "-> Getting data from TRAINING folder  4\n",
      "-> Getting data from TRAINING folder  5\n",
      "-> Getting data from TRAINING folder  6\n",
      "-> Getting data from TRAINING folder  7\n",
      "-> Getting data from TRAINING folder  8\n",
      "-> Getting data from TRAINING folder  9\n",
      "-> Getting data from TRAINING folder  10\n",
      "-> TRAIN dataset size:  2483\n",
      "-> Getting data from TESTING folder  [1]\n",
      "-> TEST dataset size:  288\n",
      "--> Fitting model!\n",
      "Epoch 1/12\n",
      "104/104 [==============================] - 4s 34ms/step - loss: 1.2074 - accuracy: 0.6045 - val_loss: 2.4027 - val_accuracy: 0.1424\n",
      "Epoch 2/12\n",
      "104/104 [==============================] - 4s 35ms/step - loss: 0.5625 - accuracy: 0.8522 - val_loss: 2.3440 - val_accuracy: 0.3299\n",
      "Epoch 3/12\n",
      "104/104 [==============================] - 4s 35ms/step - loss: 0.3538 - accuracy: 0.9138 - val_loss: 2.6261 - val_accuracy: 0.2188\n",
      "Epoch 4/12\n",
      "104/104 [==============================] - 3s 33ms/step - loss: 0.2227 - accuracy: 0.9533 - val_loss: 2.7612 - val_accuracy: 0.2951\n",
      "Epoch 5/12\n",
      "104/104 [==============================] - 3s 34ms/step - loss: 0.1661 - accuracy: 0.9642 - val_loss: 3.1512 - val_accuracy: 0.2535\n",
      "Epoch 6/12\n",
      "104/104 [==============================] - 3s 34ms/step - loss: 0.1093 - accuracy: 0.9815 - val_loss: 3.1290 - val_accuracy: 0.2569\n",
      "Epoch 7/12\n",
      "104/104 [==============================] - 3s 34ms/step - loss: 0.0902 - accuracy: 0.9839 - val_loss: 3.0620 - val_accuracy: 0.2986\n",
      "Epoch 8/12\n",
      "104/104 [==============================] - 4s 36ms/step - loss: 0.0683 - accuracy: 0.9891 - val_loss: 2.8664 - val_accuracy: 0.3681\n",
      "Epoch 9/12\n",
      "104/104 [==============================] - 4s 37ms/step - loss: 0.0532 - accuracy: 0.9944 - val_loss: 3.1548 - val_accuracy: 0.2743\n",
      "Epoch 10/12\n",
      "104/104 [==============================] - 3s 33ms/step - loss: 0.0425 - accuracy: 0.9948 - val_loss: 3.5139 - val_accuracy: 0.2292\n",
      "Epoch 11/12\n",
      "104/104 [==============================] - 3s 33ms/step - loss: 0.0315 - accuracy: 0.9984 - val_loss: 3.5309 - val_accuracy: 0.2812\n",
      "Epoch 12/12\n",
      "104/104 [==============================] - 3s 32ms/step - loss: 0.0305 - accuracy: 0.9972 - val_loss: 3.3041 - val_accuracy: 0.3021\n"
     ]
    }
   ],
   "source": [
    "### Train and evaluate via 10-audiosSubPaths cross-validation ###\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "allConfusionMatrix_AM = [] # List containing all confusion matrix for all K-Fold Train-Test\n",
    "allFitResults_A = []\n",
    "\n",
    "# Loop to traint-test in all folder combinations.\n",
    "idx = 0\n",
    "for train_index, test_index in kf.split(audiosSubPaths):\n",
    "    idx= idx+1\n",
    "    \n",
    "    print('--> Starting a loop!')\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "\n",
    "    # ---------------- STEP 1 ----------------\n",
    "    # Load training data from 9 out of 10 folders \n",
    "    # Loop through all training folders and gather data in single feature array\n",
    "    for idxTrainFolder in train_index:\n",
    "        print('-> Getting data from TRAINING folder ', idxTrainFolder+1)\n",
    "        # Read pre-saved features or segments of an audio file (pre-processed)\n",
    "        train_data = np.load(\"{0}/{1}.npz\".format(processedPath,audiosSubPaths[idxTrainFolder]), allow_pickle=True)\n",
    "\n",
    "        # Get the 'features' and 'labels' from current train folder\n",
    "        features = np.concatenate(train_data[\"features\"], axis=0) \n",
    "        labels = np.concatenate(train_data[\"labels\"], axis=0)\n",
    "\n",
    "        # Append all the 'features' and 'labels' train datasets \n",
    "        # in a single list containing all train folders data\n",
    "        x_train.append(features)\n",
    "        y_train.append(labels)\n",
    "\n",
    "    # Stack all the segments as if they are 'individual' features\n",
    "    x_train = np.concatenate(x_train, axis = 0).astype(np.float32)\n",
    "    y_train = np.concatenate(y_train, axis = 0).astype(np.float32)\n",
    "    print('-> TRAIN dataset size: ', len(x_train));\n",
    "    \n",
    "    # ---------------- STEP 2 ----------------\n",
    "    # Load test data from 1 out of 10 folders \n",
    "    # Load test data from the test folder\n",
    "    print('\\n-> Getting data from TESTING folder ', test_index+1)\n",
    "    test_data = np.load(\"{0}/{1}.npz\".format(processedPath, audiosSubPaths[test_index][0]), allow_pickle=True)\n",
    "    x_test = np.concatenate(test_data[\"features\"], axis = 0).astype(np.float32)\n",
    "    y_test = np.concatenate(test_data[\"labels\"], axis = 0).astype(np.float32)\n",
    "    print('-> TEST dataset size: ', len(x_test));\n",
    "\n",
    "    CNN_model = CNN_modelDefinition()\n",
    "    \n",
    "    print('\\n--> Fitting model!')\n",
    "    allFitResults_A.append(CNN_model.fit(x_train, y_train,validation_data=(x_test, y_test), epochs = 12, batch_size = 24, verbose = 1))\n",
    "    \n",
    "    # Predict results from test data\n",
    "    y_test_pred = [];\n",
    "    y_test_pred = CNN_model.predict(x_test); # Categorical results\n",
    "    y_test_pred = np.asarray( tf.argmax(y_test_pred, axis=1)  ) # OHE to Category\n",
    "\n",
    "    # Early stop for reduced mode\n",
    "    if REDUCED_MODE == 1:\n",
    "        break;\n",
    "\n",
    "    # Append the confusion matrix of this K-Fold run to a matirx list\n",
    "    allConfusionMatrix_AM.append(confusion_matrix(y_test_pred, y_test))\n",
    "\n",
    "\n",
    "# Save the resulting confusion matrices\n",
    "confusionPath = \"UrbanSounds8K/3_confusionMatrix/\"\n",
    "confusionFile = \"confusion_matrix\"\n",
    "np.savez(\"{0}{1}\".format(confusionPath, confusionFile), allConfusionMatrix=allConfusionMatrix_AM);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class identifiers according to UrbanSounds 8k :\n",
    "\n",
    "0 = air_conditioner\n",
    "1 = car_horn\n",
    "2 = children_playing\n",
    "3 = dog_bark\n",
    "4 = drilling\n",
    "5 = engine_idling\n",
    "6 = gun_shot\n",
    "7 = jackhammer\n",
    "8 = siren\n",
    "9 = street_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the resulting confusion matrices\n",
    "confusionPath = \"UrbanSounds8K/3_confusionMatrix/\"\n",
    "confusionFile = \"confusion_matrix\"\n",
    "np.savez(\"{0}{1}\".format(confusionPath, confusionFile), allConfusionMatrix=allConfusionMatrix_AM);\n",
    "\n",
    "#allConfusionMatrix_AM[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SZM1JVL\\Documents\\PESSOAL - MBA DSA TCC\\Convolutional Neural Network.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SZM1JVL/Documents/PESSOAL%20-%20MBA%20DSA%20TCC/Convolutional%20Neural%20Network.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m allConfusionMatrix_AM[\u001b[39m0\u001b[39;49m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "allConfusionMatrix_AM[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(allConfusionMatrix_AM[0], cmap='binary', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3c40342076cd490130f9df38b51275377a8f6bece07be498ddb041bf7f78465"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
